import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

input_texts = ['hi', 'how are you', 'what is your name']
target_texts = ['<start> hello', '<start> I am fine', '<start> I am a chatbot']

tokenizer_in = Tokenizer(filters='')
tokenizer_out = Tokenizer(filters='')
tokenizer_in.fit_on_texts(input_texts)
tokenizer_out.fit_on_texts(target_texts)

input_seq = tokenizer_in.texts_to_sequences(input_texts)
target_seq = tokenizer_out.texts_to_sequences(target_texts)

max_len_in = max(len(seq) for seq in input_seq)
max_len_out = max(len(seq) for seq in target_seq)

input_seq = pad_sequences(input_seq, maxlen=max_len_in, padding='post')
target_seq = pad_sequences(target_seq, maxlen=max_len_out, padding='post')

input_vocab_size = len(tokenizer_in.word_index) + 1
target_vocab_size = len(tokenizer_out.word_index) + 1

class Encoder(tf.keras.Model):
    def __init__(self, vocab_size, embedding_dim, enc_units):
        super().__init__()
        self.enc_units = enc_units
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
        self.lstm = tf.keras.layers.LSTM(enc_units, return_sequences=True, return_state=True)
    
    def call(self, x):
        x = self.embedding(x)
        output, h, c = self.lstm(x)
        return output, h, c

class Attention(tf.keras.layers.Layer):
    def __init__(self, units):
        super().__init__()
        self.W1 = tf.keras.layers.Dense(units)
        self.W2 = tf.keras.layers.Dense(units)
        self.V = tf.keras.layers.Dense(1)
    
    def call(self, query, values):
        query = tf.expand_dims(query, 1)
        score = self.V(tf.nn.tanh(self.W1(query) + self.W2(values)))
        attention_weights = tf.nn.softmax(score, axis=1)
        context_vector = attention_weights * values
        context_vector = tf.reduce_sum(context_vector, axis=1)
        return context_vector, attention_weights

class Decoder(tf.keras.Model):
    def __init__(self, vocab_size, embedding_dim, dec_units):
        super().__init__()
        self.dec_units = dec_units
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
        self.lstm = tf.keras.layers.LSTM(dec_units, return_sequences=True, return_state=True)
        self.fc = tf.keras.layers.Dense(vocab_size)
        self.attention = Attention(dec_units)
    
    def call(self, x, enc_output, state_h, state_c):
        context_vector, _ = self.attention(state_h, enc_output)
        x = self.embedding(x)
        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)
        output, h, c = self.lstm(x, initial_state=[state_h, state_c])
        output = self.fc(output)
        return output, h, c

embedding_dim = 256
units = 512

encoder = Encoder(input_vocab_size, embedding_dim, units)
decoder = Decoder(target_vocab_size, embedding_dim, units)

optimizer = tf.keras.optimizers.Adam()
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

@tf.function
def train_step(inp, targ):
    loss = 0
    with tf.GradientTape() as tape:
        enc_output, enc_h, enc_c = encoder(inp)
        dec_input = tf.expand_dims([tokenizer_out.word_index['<start>']] * inp.shape[0], 1)
        dec_h, dec_c = enc_h, enc_c
        for t in range(1, targ.shape[1]):
            predictions, dec_h, dec_c = decoder(dec_input, enc_output, dec_h, dec_c)
            loss += loss_object(targ[:, t], predictions[:, 0])
            dec_input = tf.expand_dims(targ[:, t], 1)
    batch_loss = loss / int(targ.shape[1])
    variables = encoder.trainable_variables + decoder.trainable_variables
    gradients = tape.gradient(loss, variables)
    optimizer.apply_gradients(zip(gradients, variables))
    return batch_loss

EPOCHS = 100
for epoch in range(EPOCHS):
    loss = train_step(input_seq, target_seq)

def evaluate(sentence):
    seq = tokenizer_in.texts_to_sequences([sentence])
    seq = pad_sequences(seq, maxlen=max_len_in, padding='post')
    enc_output, enc_h, enc_c = encoder(seq)
    dec_input = tf.expand_dims([tokenizer_out.word_index['<start>']], 0)
    dec_h, dec_c = enc_h, enc_c
    result = ''
    for _ in range(max_len_out):
        predictions, dec_h, dec_c = decoder(dec_input, enc_output, dec_h, dec_c)
        predicted_id = tf.argmax(predictions[0][0]).numpy()
        if predicted_id == 0:
            break
        word = tokenizer_out.index_word.get(predicted_id, '')
        result += word + ' '
        dec_input = tf.expand_dims([predicted_id], 0)
    return result.strip()

print(evaluate("hi"))
